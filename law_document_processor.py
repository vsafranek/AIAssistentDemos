# law_document_processor.py
"""
StrukturovanÃ½ document processor pro prÃ¡vnÃ­ dokumenty.

VyuÅ¾Ã­vÃ¡ LawJsonCrawler pro vytvoÅ™enÃ­ sÃ©manticky konzistentnÃ­ch chunkÅ¯
mÃ­sto jednoduchÃ©ho dÄ›lenÃ­ po X znacÃ­ch.

Verze: 1.0 - StrukturovanÃ½ chunking podle paragrafÅ¯, odstavcÅ¯ a bodÅ¯
"""

import numpy as np
from typing import List, Tuple, Dict, Optional
import faiss
import json
from pathlib import Path

from akkodis_clients import client_gpt_4o, client_ada_002
from seach_law_json import LawJsonCrawler, NodePath


class LawDocumentProcessor:
    """
    Processor pro prÃ¡vnÃ­ dokumenty s inteligentnÃ­m chunkingem.

    VÃ½hody oproti standardnÃ­mu DocumentProcessor:
    - âœ… Chunky respektujÃ­ strukturu zÃ¡kona (Â§, odstavce, body)
    - âœ… KaÅ¾dÃ½ chunk mÃ¡ metadata (paragraf, odstavec, cesta)
    - âœ… Å½Ã¡dnÃ© rozÅ™ezÃ¡vÃ¡nÃ­ uprostÅ™ed vÄ›ty/paragrafu
    - âœ… KontextovÃ© informace pro lepÅ¡Ã­ vyhledÃ¡vÃ¡nÃ­
    """

    def __init__(self):
        # NaÄtenÃ­ embeddings clienta
        self.embed_client, self.embed_deployment = client_ada_002()
        self.chunks: List[Dict[str, any]] = []  # StrukturovanÃ© chunky s metadaty
        self.index: Optional[faiss.Index] = None
        self.embeddings_array: Optional[np.ndarray] = None
        self.crawler: Optional[LawJsonCrawler] = None

    def load_from_json(self, json_path: str) -> None:
        """
        NaÄte strukturu zÃ¡kona z JSON (vÃ½stup parse_law).

        Args:
            json_path: Cesta k JSON souboru s parsovanÃ½m zÃ¡konem
        """
        self.crawler = LawJsonCrawler(json_path)
        print(f"ğŸ“ NaÄten zÃ¡kon z: {json_path}")

    def create_structured_chunks(
        self,
        chunk_strategy: str = "paragraph",
        max_chunk_size: int = 2000,
        include_context: bool = True
    ) -> List[Dict[str, any]]:
        """
        VytvoÅ™Ã­ strukturovanÃ© chunky podle strategie.

        Args:
            chunk_strategy:
                - "paragraph": jeden chunk = jeden paragraf (Â§)
                - "article_paragraph": jeden chunk = jeden odstavec
                - "point": jeden chunk = jeden bod
                - "mixed": adaptivnÃ­ podle dÃ©lky textu
            max_chunk_size: maximÃ¡lnÃ­ dÃ©lka chunku v znacÃ­ch
            include_context: pÅ™idat kontextovÃ© informace do chunku

        Returns:
            List strukturovanÃ½ch chunkÅ¯ s metadaty
        """
        if not self.crawler:
            raise ValueError("Nejprve naÄtÄ›te JSON pomocÃ­ load_from_json()")

        chunks = []

        for node_dict, node_path in self.crawler._collect_nodes():
            # FiltrovÃ¡nÃ­ podle strategie
            if chunk_strategy == "paragraph" and node_path.node_type != "article":
                continue
            elif chunk_strategy == "article_paragraph" and node_path.node_type != "article_paragraph":
                continue
            elif chunk_strategy == "point" and node_path.node_type not in ["point", "article_paragraph"]:
                continue

            # Extrakce textu
            text = node_path.text
            if not text or len(text.strip()) < 10:  # Ignoruj prÃ¡zdnÃ©/krÃ¡tkÃ©
                continue

            # VytvoÅ™enÃ­ kontextovÃ©ho zÃ¡hlavÃ­
            context_header = ""
            if include_context:
                context_parts = []
                if node_path.part_title:
                    context_parts.append(f"ÄŒÃ¡st: {node_path.part_title}")
                if node_path.article_title:
                    context_parts.append(node_path.article_title)
                if node_path.chain_titles:
                    context_parts.extend(node_path.chain_titles)

                if context_parts:
                    context_header = " > ".join(context_parts) + "\n\n"

            # RozdÄ›lenÃ­ dlouhÃ½ch chunkÅ¯ (zachovÃ¡nÃ­ struktury)
            full_text = context_header + text

            if len(full_text) <= max_chunk_size:
                # Vejde se do jednoho chunku
                chunks.append({
                    "text": full_text,
                    "raw_text": text,
                    "article_title": node_path.article_title,
                    "part_title": node_path.part_title,
                    "node_type": node_path.node_type,
                    "human_path": node_path.human_path(),
                    "chain_titles": node_path.chain_titles,
                    "title": node_path.title
                })
            else:
                # RozdÄ›lenÃ­ na vÄ›ty (zachovÃ¡nÃ­ sÃ©mantiky)
                sentences = self._split_into_sentences(text)
                current_chunk = context_header

                for sentence in sentences:
                    if len(current_chunk) + len(sentence) <= max_chunk_size:
                        current_chunk += sentence + " "
                    else:
                        # UloÅ¾enÃ­ aktuÃ¡lnÃ­ho chunku
                        if len(current_chunk.strip()) > len(context_header):
                            chunks.append({
                                "text": current_chunk.strip(),
                                "raw_text": current_chunk.replace(context_header, "").strip(),
                                "article_title": node_path.article_title,
                                "part_title": node_path.part_title,
                                "node_type": node_path.node_type,
                                "human_path": node_path.human_path(),
                                "chain_titles": node_path.chain_titles,
                                "title": node_path.title
                            })

                        # ZaÄÃ¡tek novÃ©ho chunku s kontextem
                        current_chunk = context_header + sentence + " "

                # UloÅ¾enÃ­ poslednÃ­ho chunku
                if len(current_chunk.strip()) > len(context_header):
                    chunks.append({
                        "text": current_chunk.strip(),
                        "raw_text": current_chunk.replace(context_header, "").strip(),
                        "article_title": node_path.article_title,
                        "part_title": node_path.part_title,
                        "node_type": node_path.node_type,
                        "human_path": node_path.human_path(),
                        "chain_titles": node_path.chain_titles,
                        "title": node_path.title
                    })

        self.chunks = chunks
        print(f"âœ… VytvoÅ™eno {len(chunks)} strukturovanÃ½ch chunkÅ¯")
        return chunks

    @staticmethod
    def _split_into_sentences(text: str) -> List[str]:
        """RozdÄ›lÃ­ text na vÄ›ty (jednoduchÃ¡ heuristika)."""
        import re
        # RozdÄ›lenÃ­ na vÄ›ty (zachovÃ¡nÃ­ teÄek v ÄÃ­slech, zkratkÃ¡ch)
        sentences = re.split(r'(?<=[.!?])\s+(?=[A-ZÄŒÅ˜Å Å½ÃÃÃÃ‰ÃšÅ®])', text)
        return [s.strip() for s in sentences if s.strip()]

    def get_embedding(self, text: str) -> np.ndarray:
        """ZÃ­skÃ¡ embedding pro text pomocÃ­ Azure OpenAI."""
        try:
            response = self.embed_client.embeddings.create(
                input=text,
                model=self.embed_deployment
            )
            embedding = response.data[0].embedding
            return np.array(embedding, dtype=np.float32)
        except Exception as e:
            print(f"âš ï¸ Chyba pÅ™i vytvÃ¡Å™enÃ­ embeddingu: {e}")
            # Fallback: nÃ¡hodnÃ½ vektor
            return np.random.randn(1536).astype(np.float32)

    def create_faiss_index(
        self,
        chunk_strategy: str = "mixed",
        max_chunk_size: int = 1500,
        include_context: bool = True
    ) -> None:
        """
        VytvoÅ™Ã­ FAISS index ze strukturovanÃ½ch chunkÅ¯.

        Args:
            chunk_strategy: strategie chunkovÃ¡nÃ­
            max_chunk_size: max. velikost chunku
            include_context: zahrnout kontextovÃ© informace
        """
        # VytvoÅ™enÃ­ strukturovanÃ½ch chunkÅ¯
        if not self.chunks:
            self.create_structured_chunks(
                chunk_strategy=chunk_strategy,
                max_chunk_size=max_chunk_size,
                include_context=include_context
            )

        print("ğŸ§  VytvÃ¡Å™enÃ­ embeddings...")
        embeddings = []

        for i, chunk in enumerate(self.chunks):
            if i % 10 == 0:
                print(f"  Progress: {i}/{len(self.chunks)}")

            embedding = self.get_embedding(chunk["text"])
            embeddings.append(embedding)

        # VytvoÅ™enÃ­ FAISS indexu
        self.embeddings_array = np.array(embeddings, dtype=np.float32)
        dimension = self.embeddings_array.shape[1]

        self.index = faiss.IndexFlatL2(dimension)
        self.index.add(self.embeddings_array)

        print(f"âœ… FAISS index vytvoÅ™en: {len(self.chunks)} chunkÅ¯, dimenze {dimension}")

    def search_relevant_chunks(
        self,
        query: str,
        k: int = 5,
        filter_by_article: Optional[str] = None
    ) -> Tuple[List[Dict[str, any]], List[float]]:
        """
        VyhledÃ¡ nejrelevantnÄ›jÅ¡Ã­ chunky pro dotaz.

        Args:
            query: vyhledÃ¡vacÃ­ dotaz
            k: poÄet vÃ½sledkÅ¯
            filter_by_article: filtrovat pouze chunky z danÃ©ho paragrafu (napÅ™. "Â§ 11")

        Returns:
            (seznam chunkÅ¯ s metadaty, vzdÃ¡lenosti)
        """
        if self.index is None:
            raise ValueError("FAISS index nenÃ­ inicializovÃ¡n. Zavolejte create_faiss_index().")

        # ZÃ­skÃ¡nÃ­ embeddingu pro dotaz
        query_embedding = self.get_embedding(query)
        query_embedding = query_embedding.reshape(1, -1)

        # VyhledÃ¡nÃ­ v FAISS
        distances, indices = self.index.search(query_embedding, min(k * 3, len(self.chunks)))

        # Aplikace filtru
        results = []
        result_distances = []

        for distance, idx in zip(distances[0], indices[0]):
            if idx >= len(self.chunks):
                continue

            chunk = self.chunks[idx]

            # FiltrovÃ¡nÃ­ podle ÄlÃ¡nku
            if filter_by_article:
                if chunk.get("article_title") != filter_by_article:
                    continue

            results.append(chunk)
            result_distances.append(float(distance))

            if len(results) >= k:
                break

        return results, result_distances

    def get_chunk_statistics(self) -> Dict[str, any]:
        """VrÃ¡tÃ­ statistiky o chunkÃ¡ch."""
        if not self.chunks:
            return {}

        stats = {
            "total_chunks": len(self.chunks),
            "chunks_by_type": {},
            "chunks_by_article": {},
            "avg_chunk_length": 0,
            "min_chunk_length": float('inf'),
            "max_chunk_length": 0
        }

        total_length = 0

        for chunk in self.chunks:
            # Podle typu
            node_type = chunk.get("node_type", "unknown")
            stats["chunks_by_type"][node_type] = stats["chunks_by_type"].get(node_type, 0) + 1

            # Podle ÄlÃ¡nku
            article = chunk.get("article_title", "N/A")
            stats["chunks_by_article"][article] = stats["chunks_by_article"].get(article, 0) + 1

            # DÃ©lka
            length = len(chunk.get("text", ""))
            total_length += length
            stats["min_chunk_length"] = min(stats["min_chunk_length"], length)
            stats["max_chunk_length"] = max(stats["max_chunk_length"], length)

        stats["avg_chunk_length"] = total_length / len(self.chunks) if self.chunks else 0

        return stats

    def export_chunks(self, output_path: str) -> None:
        """Exportuje chunky do JSON pro analÃ½zu."""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.chunks, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“¥ Chunky exportovÃ¡ny do: {output_path}")


# Backward compatibility: alias pro pÅ¯vodnÃ­ pouÅ¾itÃ­
DocumentProcessor = LawDocumentProcessor
